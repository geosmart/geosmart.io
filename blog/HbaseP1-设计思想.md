---
title: Hbase设计思想

date: 2017-07-03 20:41:50

tags: [Hbase]

categories: 大数据
---

掌握一个数据库，需要从源头了解它，它是怎么设计的，这个设计是为了解决什么问题，基于这个设计有哪些权衡取舍，这样能在深入实现细节时不会感到疑惑；

<!-- more --> 

# 列数据库
## 什么是列数据库？what

### 行/列数据库的区别
* 行式数据库：按照行存储的，行式数据库擅长随机读操作不适合用于大数据。像SQL server,Oracle，mysql等传统的是属于行式数据库范畴。
* 列数据库：列式数据库从一开始就是面向大数据环境下数据仓库的数据分析而产生。

## 列数据库如何存储数据？how

## 为什么选择列数据库？why

# LSM（Log Structured Merge Trees）日志合并树

## 常见的三种存储引擎
* `哈希存储引擎`，是哈希表的持久化实现，支持增、删、改以及随机读取操作，但不支持顺序扫描，对应的存储系统为key-value存储系统。对于key-value的插入以及查询，哈希表的复杂度都是O(1)，明显比树的操作O(n)快,如果不需要有序的遍历数据，哈希表就是your Mr.Right，典型产品是Redis,Memcached;
* `B树存储引擎`是B树的持久化实现，不仅支持单条记录的增、删、读、改操作，还支持顺序扫描（B+树的叶子节点之间的指针），典型产品是RDBMS（Mysql...）;
* `LSM树`存储引擎和B树存储引擎一样，同样支持增、删、读、改、顺序扫描操作。而且通过批量存储技术规避磁盘随机写入问题。当然凡事有利有弊，LSM树和B+树相比，LSM树牺牲了部分读性能，用来大幅提高写性能，典型应用有BigTable、Hbase、Cassandra、MongoDB等NoSQL数据库;

## LSM解决了什么问题？
>磁盘的顺序读写速度很快，随机读写很慢。

现在市面上7200rpm的希捷SATA硬盘顺序读写基本都能达到300MB/s；但是随机读写却很慢，仅100 IOPS（Input/Output Operations Per Second），  
假设随机读写每次IO大小为1KB，则随机读写数据带宽为100KB/s；`顺序读写和随机读写差了三个数量级`。

针对磁盘的上述特性，应用都根据自身读写特点做一些优化。
* 比如数据库的binlog日志就是顺序写入，所以效率很高，但是缺点也比较明显，数据很难查询读取（其实binlog是用来回放恢复数据的，不存在查询读取的使用场景）；
* Mysql的innodb存储引擎底层用B+树数据结构来组织磁盘上的数据，B+树因其节点的度远大于平衡二叉树（平衡二叉树度为2），所以B+树树高很低（3~4），
    * 每一次数据的查询只需3~4次磁盘随机IO即可查找到数据（说法不太准确，其实是找到数据所在的page 16K，加载到内存中，再以二分法查找数据，内存二分查找所耗时间远小于磁盘IO，可忽略不计），效率很高；
    * 但是insert和update操作是随机的，update隐藏的含义先找到更新的primary-key，更新，调整B+树；查找primary-key的过程很高效，但是调整B+树的磁盘IO开销却很大，因此关系型数据库mysql的写效率一致饱受诟病。

那有没有一种替代B+树的数据组织模型，在`不太影响读效率的前提下，提高数据的写效率`（随机写->顺序写）?
由O'Neil提出的LSM存储模型[LSM论文](https://www.cs.umb.edu/~poneil/lsmtree.pdf)就是解决上述问题的。

>一句话：LSM是为了解决磁盘`随机写`效率低的问题；

### 机械磁盘读取时间的影响因素
1. `寻道时间`，表示磁头在不同磁道之间移动的时间。
2. `旋转延迟`，表示在磁道找到时，中轴带动盘面旋转到合适的扇区开头处。
3. `传输时间`，表示盘面继续转动，实际读取数据的时间。
>7200转/min，旋转一周需要8.33ms，寻道约10ms
所以整个磁盘读取时间在一个磁道上是10ms级的。

### 顺序读写和随机读写为什么性能差异巨大(机械磁盘)？
>顺序读写=读取一个大文件
>随机读写=读取多个小文件
1. 磁盘读写原理不同 
* 顺序读写，主要时间花费在了`传输时间`(0.03ms左右)，而这个时间两种读写可以认为是一样的。
* 随机读写，需要多次`寻道`（10ms左右）和`旋转`（5ms左右）延迟。而这个时间可能是传输时间的许多倍（近千倍）。
2. 磁盘预读 
* 顺序读写，磁盘会`预读`，预读即在读取的起始地址连续读取多个页面（现在不需要的页面也读取了，这样以后用时就不用再读取，当一个页面用到时，大多数情况下，它周围的页面也会被用到）,
* 而随机读写，因为数据没有在一起，将`预读浪费`掉了。
3. 文件系统的overhead。
* 读写一个文件之前，得一层层目录找到这个文件，以及做一堆属性、权限之类的检查。
* 写新文件时还要加上寻找磁盘可用空间的耗时。
* 对于小文件，这些时间消耗的占比就非常大了。

### 固态磁盘的读写？


## 什么是LSM？what
LSM树的设计思想非常朴素：在于对数据的修改增量保持在内存中，达到指定的限制后将这些修改操作批量写入到磁盘中，相比较于写入操作的高性能，读取需要合并内存中最近修改的操作和磁盘中历史的数据，
> 即需要先看是否在内存中，若没有命中，还要访问磁盘文件。

> 原理：把一颗大树拆分成N棵小树，数据先写入内存中，随着小树越来越大，内存的小树会flush到磁盘中。磁盘中的树定期做合并操作，合并成一棵大树，以优化读性能。

# LSM怎么解决问题？how
简单来说，就是放弃部分磁盘`读性能`来换取`写的顺序性`。
我们假设要写入一个1000个key是随机数的数据，  
对磁盘来说，最快的写入方式一定是顺序地将每一次写入都直接写入到磁盘中即可。但这样带来的问题是，没办法查询，因为每次查询一个值都需要遍历整个数据才能找到，这个读性能就太差了；  
那么如果我想获取磁盘读性能最高，应该怎么做呢？把数据全部排序就行了，B+树就是这样的结构，但B+树的写性能太差了，需要提升写，可以放弃部分磁盘读性能，怎么办呢？  

简单，那就划分很多个小的有序结构，比如有N条数据，每m个数据，在内存里排序一次，下面100个数据，再排序一次……这样依次做下去，我就可以获得N/m个有序的小的有序结构，  
在查询的时候，因为不知道这个数据到底是在哪里，所以就从最新的一个小的有序结构里做二分查找，找得到就返回，找不到就继续找下一个小有序结构，一直到找到为止。  

很容易可以看出，这样的模式，读取的时间复杂度是`(N/m)*log2(m)`，读取效率是会下降的，这就是LSM的根本思路。  
为了优化效率Hbase提供`bloomfilter`（加速每个m的检索速度），`compact`（合并小文件降低N/m）机制；

## 为什么选用LSM? why
B+索引树和log型（append）文件操作（数据库WAL日志）是数据读写的两个极端。  
* B+树读效率高而写效率差；
* log型文件操作写效率高而读效率差；

因此要在排序和log型文件操作之间做个折中，于是就引入了`Log Structured Merge Trees`模型，  
通过名称可以看出LSM既有日志型的文件操作，提升写效率，又在每个sstable中排序，保证了查询效率。

# 参考
* [浅析LSM存储模型](https://zhuanlan.zhihu.com/p/37193700)
* [Log Structured Merge Trees(LSM) 原理](https://www.open-open.com/lib/view/open1424916275249.html)

